{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "from torch import nn, Tensor\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from typing import Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# DOes basic positional encoding with dropout\n",
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        dropout: float=0.1, \n",
    "        max_seq_len: int=5000, \n",
    "        d_model: int=256,\n",
    "        ):\n",
    "\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # adapted from PyTorch tutorial\n",
    "        position = torch.arange(max_seq_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe = torch.zeros(max_seq_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic transformer for time series data inspired by the influenza paper\n",
    "class TimeSeriesTransformer(nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "        input_size: int, \n",
    "        dim_val: int=256,  \n",
    "        num_enc_layers: int=4,\n",
    "        num_dec_layers: int=4,\n",
    "        attn_heads: int=8,\n",
    "        dropout_pos_enc: float=0.1,\n",
    "        num_predicted_features: int=1\n",
    "        ): \n",
    "\n",
    "        super().__init__() \n",
    "\n",
    "        self.input_linear = nn.Linear(\n",
    "            in_features=input_size, \n",
    "            out_features=dim_val \n",
    "            )\n",
    "\n",
    "\n",
    "        # Create positional encoder\n",
    "        self.positional_encoding_layer = PositionalEncoder(\n",
    "            d_model=dim_val,\n",
    "            dropout=dropout_pos_enc\n",
    "        )\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=dim_val, \n",
    "            nhead=attn_heads,\n",
    "            )\n",
    "        \n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            encoder_layer=encoder_layer,\n",
    "            num_layers=num_enc_layers, \n",
    "            )\n",
    "\n",
    "        self.decoder_input_layer = nn.Linear(\n",
    "            in_features=num_predicted_features,\n",
    "            out_features=dim_val\n",
    "            )     \n",
    "\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=dim_val,\n",
    "            nhead=attn_heads,\n",
    "            )\n",
    "\n",
    "        self.decoder = nn.TransformerDecoder(\n",
    "            decoder_layer=decoder_layer,\n",
    "            num_layers=num_dec_layers, \n",
    "            )\n",
    "        \n",
    "        self.output_linear = nn.Linear(\n",
    "            in_features=dim_val, \n",
    "            out_features=num_predicted_features\n",
    "            )\n",
    "\n",
    "    def forward(self, src: Tensor, tgt: Tensor, src_mask: Tensor=None, \n",
    "                tgt_mask: Tensor=None) -> Tensor:\n",
    "        src = self.input_linear(src) \n",
    "        src = self.positional_encoding_layer(src) \n",
    "\n",
    "        src = self.encoder(src)\n",
    "\n",
    "        decoder_output = self.decoder_input_layer(tgt) \n",
    "        decoder_output = self.decoder(\n",
    "            tgt=decoder_output,\n",
    "            memory=src,\n",
    "            tgt_mask=tgt_mask,\n",
    "            memory_mask=src_mask\n",
    "            )\n",
    "\n",
    "        decoder_output = self.output_linear(decoder_output) \n",
    "        return decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(dim1: int, dim2: int) -> Tensor:\n",
    "    return torch.triu(torch.ones(dim1, dim2) * float('-inf'), diagonal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader for the transformer,returns a 3 tuple of input,decoder input (for training) and ground truth output.\n",
    "class TransformerDataset(Dataset):\n",
    "    def __init__(self, \n",
    "        data: torch.tensor,\n",
    "        input_len: int, \n",
    "        output_len:int\n",
    "        ):\n",
    "        \n",
    "        super().__init__()\n",
    "        print(\"Transformer Dataset: data shape={}\".format(data.size(),data.shape))\n",
    "        self.data = data\n",
    "        self.inp_len = input_len\n",
    "        self.op_len = output_len\n",
    "\n",
    "    # Overriden fucntions required for DataLoader to work\n",
    "    def __len__(self):\n",
    "        return len(self.data)-(self.inp_len+self.op_len)+1\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sequence = self.data[index:index+self.inp_len+self.op_len]\n",
    "        src, dec_src, tgt = self.get_tuple(\n",
    "            sequence=sequence,\n",
    "            enc_seq_len=self.inp_len,\n",
    "            target_seq_len=self.op_len\n",
    "            )\n",
    "        return src, dec_src, tgt\n",
    "    \n",
    "    def get_tuple(\n",
    "        self,\n",
    "        sequence: torch.Tensor, \n",
    "        enc_seq_len: int, \n",
    "        target_seq_len: int\n",
    "        ): \n",
    "        # encoder input\n",
    "        src = sequence[:enc_seq_len] \n",
    "        \n",
    "        # decoder input. The last element of the encoder input is used as the first element of the decoder input\n",
    "        dec_src = sequence[enc_seq_len-1:len(sequence)-1]\n",
    "        \n",
    "        #the ground truth\n",
    "        tgt = sequence[-target_seq_len:]\n",
    "        return src, dec_src, tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.1\n",
    "batch_size = 128\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "dim_val = 256\n",
    "attn_heads = 8\n",
    "input_len = 30 # length of input given to encoder\n",
    "\n",
    "output_sequence_length = 1 # target sequence length. If hourly data and length = 48, you predict 2 days ahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate masks outside and then use them in the training loop\n",
    "\n",
    "src_mask = generate_square_subsequent_mask(\n",
    "    dim1=output_sequence_length,\n",
    "    dim2=input_len\n",
    "    ).to(device)\n",
    "\n",
    "tgt_mask = generate_square_subsequent_mask( \n",
    "    dim1=output_sequence_length,\n",
    "    dim2=output_sequence_length\n",
    "    ).to(device)\n",
    "\n",
    "\n",
    "def train_epoch(\n",
    "    model : TimeSeriesTransformer,\n",
    "    train_loader: DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    loss_fn: nn.Module,\n",
    "    device: torch.device):\n",
    "\n",
    "    # print(\"From train_epoch() \")\n",
    "    running_loss = 0\n",
    "    model.train()\n",
    "    for i, data in enumerate(train_loader):\n",
    "        src, dec_src, tgt = data\n",
    "        optimizer.zero_grad()\n",
    "        src = src.permute(1, 0, 2).to(device)\n",
    "        dec_src = dec_src.permute(1, 0, 2).to(device)\n",
    "        tgt = tgt.permute(1, 0,2).to(device)\n",
    "        # print(\"src.shape={}, dec_src.shape={}, tgt.shape={}\".format(src.shape,dec_src.shape,tgt.shape))\n",
    "        output = model(\n",
    "            src=src,\n",
    "            tgt=dec_src,\n",
    "            src_mask=src_mask,\n",
    "            tgt_mask=tgt_mask\n",
    "            )\n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_fn(output, tgt)\n",
    "        loss.backward()\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        # if(i==5):\n",
    "        #     print(\"Batch: {}, Loss: {}\".format(i, loss.item()))\n",
    "\n",
    "    return running_loss / len(train_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_epoch(\n",
    "    model: nn.Module, \n",
    "    src: torch.Tensor, \n",
    "    forecast_window: int,\n",
    "    batch_size: int,\n",
    "    device,\n",
    "    ) -> torch.Tensor:\n",
    "\n",
    "    # last value of the encoder input is used as the first value of the decoder input\n",
    "    tgt = src[-1, :, 0] \n",
    "\n",
    "    if batch_size == 1:\n",
    "        tgt = tgt.unsqueeze(0).unsqueeze(0) \n",
    "\n",
    "    if batch_size > 1:\n",
    "        tgt = tgt.unsqueeze(0).unsqueeze(-1)\n",
    "\n",
    "    # Iteratively generate the output\n",
    "    for _ in range(forecast_window-1):\n",
    "        # This is a lazy implementation not particularly efficient\n",
    "        tgt_mask =generate_square_subsequent_mask(\n",
    "            dim1=tgt.shape[0],\n",
    "            dim2=tgt.shape[0],\n",
    "            ).to(device)\n",
    "\n",
    "        src_mask =generate_square_subsequent_mask(\n",
    "            dim1=tgt.shape[0],\n",
    "            dim2=src.shape[0],\n",
    "            ).to(device)\n",
    "\n",
    "        # Make prediction\n",
    "        pred = model(src, tgt, src_mask, tgt_mask) \n",
    "        last_predicted_value = pred[-1].unsqueeze(0)\n",
    "\n",
    "        # Update generated output\n",
    "        tgt = torch.cat((tgt, last_predicted_value.detach()), 0)\n",
    "\n",
    "    tgt_mask =generate_square_subsequent_mask(\n",
    "            dim1=tgt.shape[0],\n",
    "            dim2=tgt.shape[0],\n",
    "            ).to(device)\n",
    "\n",
    "    src_mask =generate_square_subsequent_mask(\n",
    "        dim1=tgt.shape[0],\n",
    "        dim2=src.shape[0],\n",
    "        ).to(device)\n",
    "\n",
    "    pred = model(src, tgt, src_mask, tgt_mask)\n",
    "\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(\n",
    "    model: TimeSeriesTransformer,\n",
    "    test_loader: DataLoader,\n",
    "    device: torch.device        \n",
    "):\n",
    "    actual=[]\n",
    "    pred=[]\n",
    "    with torch.no_grad():\n",
    "        for i, (src, _, tgt) in enumerate(test_loader):\n",
    "            src=src.permute(1,0,2).to(device)\n",
    "            tgt=tgt.permute(1,0,2).to(device)\n",
    "            prediction = inference_epoch(\n",
    "                model=model, \n",
    "                src=src, \n",
    "                forecast_window=1,\n",
    "                batch_size=src.shape[1],\n",
    "                device=device\n",
    "                )\n",
    "            actual.extend(tgt.squeeze().cpu().tolist())\n",
    "            pred.extend(prediction.squeeze().squeeze().cpu().tolist())\n",
    "    plt.figure(figsize=(16,8))\n",
    "    plt.plot(min_max_scalar.inverse_transform(np.array(actual).reshape(-1,1)), color='blue', label='Actual Price')\n",
    "    plt.plot(min_max_scalar.inverse_transform(np.array(pred).reshape(-1,1)) , color='red', label='Predicted Price')\n",
    "    plt.title('Stock Price Prediction')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Price')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def fit(\n",
    "    model: TimeSeriesTransformer,\n",
    "    n_epochs: int,\n",
    "    train_loader: DataLoader,\n",
    "    test_loader: DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    loss_fn: nn.Module,\n",
    "    device: torch.device\n",
    "    ):\n",
    "\n",
    "    best_valid_loss = float('inf')\n",
    "    loss_over_time = []\n",
    "    for epoch in range(n_epochs):\n",
    "        loss = train_epoch(\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            optimizer=optimizer,\n",
    "            loss_fn=loss_fn,\n",
    "            device=device\n",
    "            )\n",
    "        print(\"Epoch: {}, Loss: {}\".format(epoch+1, loss))\n",
    "        loss_over_time.append(loss)\n",
    "        with torch.no_grad():\n",
    "            loss_val = 0\n",
    "            for i, (src, _, tgt_y) in enumerate(test_loader):\n",
    "                src=src.permute(1,0,2).to(device)\n",
    "                tgt_y=tgt_y.permute(1,0,2).to(device)\n",
    "                prediction = inference_epoch(\n",
    "                    model=model, \n",
    "                    src=src, \n",
    "                    forecast_window=1,\n",
    "                    batch_size=src.shape[1],\n",
    "                    device=device\n",
    "                    )\n",
    "                loss_val += loss_fn(tgt_y, prediction)\n",
    "            loss_val = loss_val.item()/len(test_loader)\n",
    "            print(\"Validation Loss: {}\".format(loss_val))\n",
    "\n",
    "            if loss_val < best_valid_loss:\n",
    "                best_valid_loss = loss_val\n",
    "                print(f\"Best validation loss: {best_valid_loss}\")\n",
    "                print(f\"Saving best model for epoch: {epoch+1}\\n\")\n",
    "                torch.save({\n",
    "                    'epoch': epoch+1,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': loss_val,\n",
    "                    }, 'outputs/best_model.pth')\n",
    "    return loss_over_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = TimeSeriesTransformer(\n",
    "    input_size=1,\n",
    "    num_predicted_features=1\n",
    "    ).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Loading financial data\n",
    "import yfinance as yf\n",
    "apple = yf.Ticker(\"ibm\")\n",
    "df = apple.history(start='2001-01-19', end='2022-05-13', actions=False)\n",
    "df = df[['Close']]\n",
    "print(df[-5:])\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.title('Close Price History')\n",
    "plt.plot(df['Close'])\n",
    "plt.xlabel('Date', fontsize=18)\n",
    "plt.ylabel('Close Price INR', fontsize=18)\n",
    "plt.show()\n",
    "data=df.values\n",
    "\n",
    "min_max_scalar = MinMaxScaler(feature_range=(0,1))\n",
    "data = min_max_scalar.fit_transform(data)\n",
    "\n",
    "training_data = data[:-(round(len(data)*test_size))]\n",
    "testing_data = data[-(round(len(data)*test_size)):]\n",
    "\n",
    "\n",
    "# Making dataset\n",
    "training_data = TransformerDataset(\n",
    "    data=torch.tensor(training_data).float(),\n",
    "    input_len=input_len,\n",
    "    output_len=output_sequence_length\n",
    "    )\n",
    "\n",
    "testing_data=TransformerDataset(\n",
    "    data=torch.tensor(testing_data).float(),\n",
    "    input_len=input_len,\n",
    "    output_len=output_sequence_length\n",
    "    )\n",
    "\n",
    "# Making dataloader\n",
    "training_data = DataLoader(training_data, batch_size)\n",
    "testing_data = DataLoader(testing_data, batch_size)\n",
    "\n",
    "# Untrained model prediction\n",
    "predict(model,testing_data,device)\n",
    "pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
    "print(pytorch_total_params)\n",
    "training_losses=fit(\n",
    "    model=model,\n",
    "    n_epochs=100,\n",
    "    train_loader=training_data,\n",
    "    test_loader=testing_data,\n",
    "    optimizer=optimizer,\n",
    "    loss_fn=loss_fn,\n",
    "    device=device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_cp = torch.load('outputs/best_model.pth')\n",
    "best_model_epoch = best_model_cp['epoch']\n",
    "print(f\"Best model was saved at {best_model_epoch} epochs\\n\")\n",
    "model.load_state_dict(best_model_cp['model_state_dict'])\n",
    "print(best_model_cp['loss'])\n",
    "predict(model,testing_data,device)\n",
    "# print(\"Inference time taken is \",end-start)\n",
    "# print(training_losses)\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.title('Loss vs epochs')\n",
    "plt.plot(training_losses)\n",
    "plt.xlabel('Epochs', fontsize=18)\n",
    "plt.ylabel('Training loss', fontsize=18)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
